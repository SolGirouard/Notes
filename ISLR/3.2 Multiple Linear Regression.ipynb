{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Multiple Linear Regression\n",
    "\n",
    "Simple linear regression is a useful approach for predicting a response on the basis of a single predictor variable.\n",
    "\n",
    "However, in practice we often have more than one predictor.\n",
    "\n",
    "For example, in the Advertising data, we have examined the relationship between sales and TV advertising. We also have data for the amount spent on radio and newspaper advertising and we may want to know whether either of these two media is associated with sales.\n",
    "\n",
    "We could run three separate simple linear regressions, but:\n",
    "* It is unclear how to make a single prediction of sales given levels of 3 different budgets, since each of the budgets is associated with a separate regression equation\n",
    "* Each of the regression equations ignors the other 2 media in forming estimates for regression coefficients.\n",
    "\n",
    "Instead of fitting separate simple linear regression models for each predictor, a better approach is to extend the simple linear regression model so that it can directly accommodate multiple predictors.\n",
    "\n",
    "We can do this by giving each predictor, p, a separate slope coefficient in a single model:\n",
    "\n",
    "$ Y = \\beta_0 + \\beta_{1}X_1 + \\beta_{2}X_2 + ... + \\beta_{p}X_p + \\epsilon $\n",
    "\n",
    "where $X_j$ represents the jth predictor and $\\beta_j$ quantifies the association between that variable and the response.\n",
    "\n",
    "We interpret $\\beta_j$  as the average effect on Y of one unit increase in $X_j$, holding all other predictors fixed.\n",
    "\n",
    "In the advertising example this becomes:\n",
    "\n",
    "$ sales = \\beta_0 + \\beta_1 \\times TV + \\beta_2 \\times radio + \\beta_3 \\times newspaper + \\epsilon $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1. Estimating the Regression Coefficients\n",
    "\n",
    "As was the case in the simple linear regression setting, the regression coefficients $\\beta_0, \\beta_{1}, \\beta_{2}, ..., \\beta_{p} $ are unknown and must be estimated. Given estimates $\\hat{\\beta_0}, \\hat{\\beta_{1}}, \\hat{\\beta_{2}}, ..., \\hat{\\beta_{p}} $ we make predictions using the formula:\n",
    "\n",
    "$ \\hat{y} = \\hat{\\beta_0}, \\hat{\\beta_{1}}x_1, \\hat{\\beta_{2}}x_2, ..., \\hat{\\beta_{p}}x_p $  \n",
    "\n",
    "The parameters are estimated using the same least squares approach that we saw in the context of simple linear regression. We choose $\\hat{\\beta_0}, \\hat{\\beta_{1}}, \\hat{\\beta_{2}}, ..., \\hat{\\beta_{p}} $ to minimuse the sum of squared residuals.\n",
    "\n",
    "$ RSS = \\Sigma^n_{i=1}(y_i - \\hat{y}_i)^2 $  \n",
    "\n",
    "$ RSS = \\Sigma^n_{i=1}(y_i - \\hat{\\beta_0} - \\hat{\\beta_{1}}x_1 - \\hat{\\beta_{2}}x_2 - ... - \\hat{\\beta_{p}}x_p) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://raw.githubusercontent.com/benalexkeen/Notes/master/ISLR/input_figures/Fig_3-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These coefficient estimates have complicated forms that are more easily represented using matrix algebra. Any statistical software package can be used to compute these coefficient estimates, and later in this chapter we will show how this can be done in R.\n",
    "\n",
    "The figure above illustrates an example of the least squares fit to a toy data set with p = 2 predictors.\n",
    "\n",
    "In the advertising data set, doing 3 simple linear regressions showed a significant effect of increasing sales given an increase on newspaper advertising budget (p value <0.0001). However, in a multiple linear regression, there is no significant increase in sales (p value 0.8599). This is because the multiple linear regression coefficient represents the average effect of increasing newspaper spending by \\$1000 _while holding TV and radio fixed_.\n",
    "\n",
    "An increase in newspaper spending is actually correlated with an increase in radio and TV spending, not with an increase in sales, as shown by the correlation matrix below and it is those that cause the increase in sales:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Table 3.5](https://raw.githubusercontent.com/benalexkeen/Notes/master/ISLR/input_figures/Table_3-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 Some Important Questions\n",
    "\n",
    "When we perform multiple linear regression, we usually are interested in answering a few important questions.\n",
    "\n",
    "1. Is at least one of the predictors $ X_1, X_2, ..., X_p $ useful in predicting the response?\n",
    "2. Do all the predictors help explain Y, or is only a subset of the predictors useful?\n",
    "3. How well does the model fit the data?\n",
    "4. Given a set of predictor values, what response value should we predict, and how accurate is our prediction?\n",
    "\n",
    "We now address each of these questions in turn.\n",
    "\n",
    "### One: Is there a relationship between the response and the predictors?\n",
    "\n",
    "Recall that in the simple linear regression setting, in order to determine whether there is a relationship between the response and the predictor, we can simple check whether $\\beta_1 = 0$.\n",
    "\n",
    "In multiple linear regression, we need to ask whether all of the regression coefficients are zero.\n",
    "\n",
    "We test the null hypothesis:\n",
    "\n",
    "$ H_0: \\beta_1 = \\beta_2 = ... \\beta_p = 0 $  \n",
    "\n",
    "versus the alternative:\n",
    "\n",
    "$ H_a: at\\ least\\ one\\ \\beta_j\\ is\\ non-zero $\n",
    "\n",
    "This hypothesis test is performed by computing the F-statistic:\n",
    "\n",
    "$ F = \\dfrac{(TSS - RSS) / p}{RSS/(n-p-1)} $\n",
    "\n",
    "Where, p is the number of predictors and, as it is with simple linear regression:\n",
    "\n",
    "$ TSS = \\Sigma^n_{i=1}(y_i - \\bar{y})^2 $\n",
    "\n",
    "$ RSS = \\Sigma^n_{i=1}(y_i - \\hat{y})^2 $\n",
    "\n",
    "If a linear model assumptions are correct, one can show that:\n",
    "$ E{RSS/(n-p-1} = \\sigma^2 $\n",
    "and that, provided H<sub>0</sub> is true:\n",
    "$ E{(TSS-RSS) / p} = \\sigma^2 $\n",
    "\n",
    "Hence, when there is no relationship between the response and predictors, we would expect the F statistic to take on a value close to 1.\n",
    "\n",
    "On the other hand if $H_a$ is true, then $ E{(TSS-RSS) / p} > \\sigma^2 $, so we expect F to be greater than 1\n",
    "\n",
    "How large an F statistic is needed? For large n, a small F (slightly more than 1) might be significant.\n",
    "You can use a statistical software package to determine the p-value for F. \n",
    "\n",
    "Based on this p value, we can determine whether or not to reject H<sub>0</sub>\n",
    "\n",
    "When calculating t values for effect of the individual predictors, these t values are calculated by working out the F statistic when including the predictor and when excluding the predictor.\n",
    "\n",
    "This really only works when the number of observations (n) is greater than the number of predictors (p). In Chapter 6, different approaches to selecting predictors are discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two: Deciding on Important Variables\n",
    "\n",
    "If we get a p-value that demonstrates at least one of the predictors is related to the response, it is natural to wonder which of the predictors are really at cause. \n",
    "\n",
    "The task of determining which predictors are associated with the response is referred to as variable selection.\n",
    "\n",
    "Ideally, this would be performed by trying out different models with a different subset of predictors. E.g. if p=2 then we could try out:\n",
    "1. A model containing no variables\n",
    "2. A model containing X<sub>1</sub> only\n",
    "3. A model containing X<sub>2</sub> only\n",
    "4. A model containing both X<sub>1</sub> and X<sub>2</sub>\n",
    "and then select the best model out of all the models we have considered.  \n",
    "\n",
    "How do we determine which model is best? Various statistics can be used to judge the quality of a model, including:\n",
    "* Mallow's C<sub>p</sub>\n",
    "* Akaike information criterion (AIC)\n",
    "* Bayesian information criterion\n",
    "* Adjusted R<sup>2</sup>\n",
    "These are discussed in more detail in chapter 6.\n",
    "\n",
    "However, even for moderate p values, trying out every possible subset of predictors is infeasible. Therefore, unless p is very small, we cannot consider all 2<sup>p</sup> models and need an automated and efficient approach to choose a smaller set of models to consider.\n",
    "\n",
    "There are 3 classical approaches for this task:\n",
    "* __Forward selection__ - Begin with the null model, an intercept with no predictors. Then fit p simple linear regressions and add to the null model the variable that results in the lowest RSS. We then add to that model the variable that results in the lowest RSS for the new two-variable model. Continue until some stopping rule is satisfied.\n",
    "\n",
    "* __Backward selection__ - Start with all variables and remove variables with the largest p-value - that is, the variable that is the least statistically significant. The new (p - 1) variable model is fit and the variable with the largest p-value is removed. This procedure continues until a stopping rule is reached. For instance, we may stop when all remaining variables have a p-value below some threshold.\n",
    "\n",
    "* __Mixed selection__ - A combination of forward and backward selection. We start with no variables in the model and add the variable that provides the best fit. If the p-value for one of the variables in the model rises above a certain threshold, then we remove that variable from the model.\n",
    "\n",
    "Backward selection cannot be used if p > n (number of predictors > number of observations), while forward selection can also be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three: How well does the model fit the data\n",
    "\n",
    "Two of the most common numerical measures of model fit are the RSE and the R<sup>2</sup>, the fraction of variance explained. \n",
    "\n",
    "These quantities are computed and interpreted int he same fashion as for simple linear regression.\n",
    "\n",
    "Recall that in simple regression, R<sup>2</sup> is the square of the correlation. In multiple linear regression it equals $ Cor(Y, \\hat{Y})^2 $, the square of the correlation between the response and the fitted linear model.\n",
    "\n",
    "An R<sup>2</sup> close to 1 indicates that the model explains a large portion of the variance in the response variable. \n",
    "\n",
    "R<sup>2</sup> will always increase as more variables are added to the model, even if those variables are only weakly associated with the response. This is due to the fact that adding another variable to the least squares equation must allow us to fit the training data (though not necessarily the testing data) more accurately. \n",
    "\n",
    "We want to look for large increases in R<sup>2</sup> as opposed to small increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four: Given a set of predictor values, what response value should we predict, and how accurate is our prediction?\n",
    "\n",
    "Once we have fit the multiple regression model, it is straightforward to apply our equation:\n",
    "\n",
    "$ \\hat{y} = \\hat{\\beta_0}, \\hat{\\beta_{1}}x_1, \\hat{\\beta_{2}}x_2, ..., \\hat{\\beta_{p}}x_p $  \n",
    "\n",
    "to a new observation (X) to predict the response Y.\n",
    "\n",
    "However, there are three sorts of uncertainty associated with this prediction:\n",
    "\n",
    "1. The coefficient estimates $\\hat{\\beta_0}, \\hat{\\beta_1}, ... \\hat{\\beta_p}$ are only estimates for ${\\beta_0}, {\\beta_1},...{\\beta_p}$. That is to say the least squares plan is only an estimate fro the true population regression plane, and there is an error related to the reducible error. We can compute a confidence interval in order to determine how close $\\hat{Y}$ will be to $ f(X) $.\n",
    "2. Of course, in practice, a linear model is almost always an approximation of reality. So we have a _model bias_.\n",
    "3. Even if we knew $ f(X) $ - that is even if we knew ${\\beta_0}, {\\beta_1},...{\\beta_p}$ perfectly, the response value cannot be predicted perfectly because of random error $\\epsilon$ in the model.\n",
    "\n",
    "We use a confidence interval to quantify uncertainty. \n",
    "\n",
    "For example in our sales example, given that \\$100,000 is spent on TV advertising and \\$20,000 is spent on radio advertising, the 95% confidence interval is [10,985, 11,528]. We interpret this to mean that 95% of intervals of this form will contain the true value of $f(X)$.\n",
    "\n",
    "On the other hand, a prediction interval can be used to quantify the uncertainty surrounding sales for a particular city. If this confidence interval is wide, we are less certain about sales given two values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
